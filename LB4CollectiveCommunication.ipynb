{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X67Q-9A8jRiV",
        "outputId": "cad51d2c-165d-49c6-a5f2-0e82b1490c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp39-cp39-linux_x86_64.whl size=3380655 sha256=972eb29a3a7ac94a61750d6dff4d7509de46cf850c591184bd789a4192c3294a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/81/9f/43a031fce121c845baca1c5d9a1468cad98208286aa2832de9\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-R8AL1qGSOF",
        "outputId": "5bb85d33-7e31-4605-f0a0-6c9e254f0002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUlDg57uif8i"
      },
      "source": [
        "# Collective communication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eVvy6xyhrc0",
        "outputId": "aa07fc36-1853-415b-fa6e-e4c25b8412c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing LB4CollectiveCommunication.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LB4CollectiveCommunication.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timeit\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "\n",
        "def create_clusters(centroids, num_clusters, data):\n",
        "    num_data_points, _ = np.shape(data)\n",
        "    cluster_idx = np.empty(num_data_points)\n",
        "    for point in range(num_data_points):\n",
        "        cluster_idx[point] = closest_centroid(data[point], centroids, num_clusters)\n",
        "    return cluster_idx\n",
        "\n",
        "def compute_means(cluster_idx, num_clusters, data):\n",
        "    centroids = np.empty((num_clusters, data.shape[1]))\n",
        "    for k in range(num_clusters):\n",
        "        cluster_points = data[cluster_idx == k] \n",
        "        centroids[k] = np.mean(cluster_points, axis=0)\n",
        "    return centroids\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    squared_distance = np.sum(np.power(x1 - x2, 2))\n",
        "    distance = np.sqrt(squared_distance)\n",
        "    return distance\n",
        "\n",
        "def closest_centroid(data, centroids, num_clusters):\n",
        "    distances = [euclidean_distance(data, centroid) for centroid in centroids]\n",
        "    closest_idx = np.argmin(distances)\n",
        "    return closest_idx  \n",
        "\n",
        "def initialize_random_centroids(num_clusters, data):\n",
        "    m, n = np.shape(data)\n",
        "    centroids = data[np.random.choice(m, size=num_clusters, replace=False)]\n",
        "    return centroids\n",
        "\n",
        "def run_Kmeans(num_clusters, data, max_iterations=50):\n",
        "    centroids = initialize_random_centroids(num_clusters, data)\n",
        "    for _ in range(max_iterations):\n",
        "        clusters = create_clusters(centroids, num_clusters, data)\n",
        "        previous_centroids = centroids\n",
        "        centroids = compute_means(clusters, num_clusters, data)\n",
        "        if np.allclose(previous_centroids, centroids):\n",
        "            return clusters, centroids\n",
        "    return clusters, centroids\n",
        "\n",
        "def score_within_cluster_dispersion(cluster, data_clusters):\n",
        "    cluster_data = data_clusters[cluster]\n",
        "    cluster_size = cluster_data.shape[0]\n",
        "    cluster_dispersion = np.sum(np.var(cluster_data, ddof=0, axis=0))\n",
        "    return cluster_size * cluster_dispersion\n",
        "\n",
        "def calculate_index(data):\n",
        "    data_clusters = {}\n",
        "    data_features = data.copy()    \n",
        "    label_target = 'XCoord'\n",
        "    data_target = data_features.pop(label_target)\n",
        "    labels_clusters = np.unique(data_target)\n",
        "    num_clusters = len(labels_clusters)\n",
        "\n",
        "    data_frame = data.copy()\n",
        "\n",
        "    for cluster in labels_clusters:\n",
        "        data_clusters[cluster] = data_frame[data_frame[label_target] == cluster].drop(columns=label_target)\n",
        "\n",
        "    num_observation_for_specific_cluster = {cluster: len(data_clusters[cluster])\n",
        "             for cluster in labels_clusters}\n",
        "    \n",
        "    B = pd.DataFrame()\n",
        "    data_centroids = data_frame.groupby(by=label_target).mean().T\n",
        "    data_barycenter = data_features.mean()\n",
        "\n",
        "    for cluster in labels_clusters:\n",
        "        B = B.append(np.sqrt(num_observation_for_specific_cluster[cluster]) *\n",
        "        (data_centroids[cluster] - data_barycenter), ignore_index=True)\n",
        "    \n",
        "    scatter_matrix_between_group_BG = B.T.dot(B)\n",
        "\n",
        "    score_between_group_dispersion = np.trace(scatter_matrix_between_group_BG)\n",
        "\n",
        "    BGSS_red = score_between_group_dispersion / (num_clusters - 1)\n",
        "\n",
        "    score_pooled_within_cluster_dispersion = np.sum([score_within_cluster_dispersion(cluster, data_clusters) for cluster in labels_clusters])\n",
        "    \n",
        "    num_observations = len(data_features)\n",
        "    WGSS_red = score_pooled_within_cluster_dispersion / (num_observations - num_clusters)\n",
        "\n",
        "    index = BGSS_red / WGSS_red\n",
        "\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    start_timer = timeit.default_timer()   \n",
        "\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    send_size = 0\n",
        "    recv_size = 0\n",
        "    send_buf = None\n",
        "    recv_buf = None\n",
        "    data = None \n",
        "    centroids = None\n",
        "\n",
        "    if rank == 0:\n",
        "        data = pd.read_csv('/content/drive/MyDrive/Datasets/brooklyn_sales_map.csv', low_memory=False)\n",
        "        data.drop(data.columns.difference(['XCoord','YCoord']), 1, inplace=True)\n",
        "        df = data[data.XCoord.notnull()]\n",
        "        coordinates = df.head(1000)\n",
        "\n",
        "        send_buf = coordinates\n",
        "        send_size = coordinates.size\n",
        "        Mind = np.array(size, dtype='d')\n",
        "        Kmean = np.array(size, dtype='d')\n",
        "            \n",
        "    send_size = comm.bcast(send_size, root = 0)\n",
        "    send_buf = comm.bcast(send_buf, root = 0)\n",
        "    mind = 0\n",
        "\n",
        "    if rank != 0:\n",
        "        n = int(send_size / size)\n",
        "        scaler = MinMaxScaler()\n",
        "        rndprem = np.random.permutation(send_buf.shape[0])\n",
        "        send_buf = send_buf.iloc[rndprem[0:500], :].copy()\n",
        "        coordinates = send_buf\n",
        "        send_buf = scaler.fit_transform(send_buf)\n",
        "        kmeans, centroids = run_Kmeans(4, send_buf)\n",
        "        mind = calculate_index(coordinates)\n",
        "\n",
        "    Mind = comm.gather(mind, root=0)\n",
        "    Kmean = comm.gather(centroids, root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        max_val = 0\n",
        "        max_index = 0\n",
        "        for i in range(size - 1):\n",
        "            if (Mind[i] > max_val):\n",
        "                max_index = i\n",
        "                max_val = Mind[i]\n",
        "\n",
        "        print('Max index: ', max_index)\n",
        "        print('Max value: ',  max_val)\n",
        "        \n",
        "        centroid = Kmean[max_index]\n",
        "        print('Centroids: ', centroid) \n",
        "\n",
        "    time = timeit.default_timer() - start_timer\n",
        "    print('Running time: {:2.4f} sec'.format(time))\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeatIDyHmGk8",
        "outputId": "6ab98942-e154-4387-a90c-96f4b8d0308f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running time: 87.6052 sec\n",
            "Running time: 86.6113 sec\n",
            "Running time: 89.4801 sec\n",
            "Running time: 88.5526 sec\n",
            "Running time: 89.4020 sec\n",
            "Running time: 90.8041 sec\n",
            "Running time: 91.1600 sec\n",
            "Max index:  6\n",
            "Max value:  202.32017322826792\n",
            "Centroids:  [[0.9749057  0.86584511]\n",
            " [0.97373962 0.94081565]\n",
            " [0.         0.        ]\n",
            " [0.97329076 0.75612901]]\n",
            "Running time: 91.2745 sec\n"
          ]
        }
      ],
      "source": [
        "! mpirun -n 8 --allow-run-as-root --oversubscribe python LB4CollectiveCommunication.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}