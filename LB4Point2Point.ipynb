{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X67Q-9A8jRiV",
        "outputId": "01b35118-6a95-4132-9da4-64f32ef7fbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp39-cp39-linux_x86_64.whl size=3380630 sha256=6017256dd0fe6b7dbf2307b3438e71062c5bb0d10b9b9740452ba88c6d1fb128\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/81/9f/43a031fce121c845baca1c5d9a1468cad98208286aa2832de9\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-R8AL1qGSOF",
        "outputId": "c327c98d-63f9-41d8-b254-bfb8c7ec8680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUlDg57uif8i"
      },
      "source": [
        "# Point to Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eVvy6xyhrc0",
        "outputId": "49476c42-1219-41f5-8534-97be6c04de8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing LB4Point2Point.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LB4Point2Point.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timeit\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "\n",
        "def create_clusters(centroids, num_clusters, data):\n",
        "    num_data_points, _ = np.shape(data)\n",
        "    cluster_idx = np.empty(num_data_points)\n",
        "    for point in range(num_data_points):\n",
        "        cluster_idx[point] = closest_centroid(data[point], centroids, num_clusters)\n",
        "    return cluster_idx\n",
        "\n",
        "def compute_means(cluster_idx, num_clusters, data):\n",
        "    centroids = np.empty((num_clusters, data.shape[1]))\n",
        "    for k in range(num_clusters):\n",
        "        cluster_points = data[cluster_idx == k] \n",
        "        centroids[k] = np.mean(cluster_points, axis=0)\n",
        "    return centroids\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    squared_distance = np.sum(np.power(x1 - x2, 2))\n",
        "    distance = np.sqrt(squared_distance)\n",
        "    return distance\n",
        "\n",
        "def closest_centroid(data, centroids, num_clusters):\n",
        "    distances = [euclidean_distance(data, centroid) for centroid in centroids]\n",
        "    closest_idx = np.argmin(distances)\n",
        "    return closest_idx  \n",
        "\n",
        "def initialize_random_centroids(num_clusters, data):\n",
        "    m, n = np.shape(data)\n",
        "    centroids = data[np.random.choice(m, size=num_clusters, replace=False)]\n",
        "    return centroids\n",
        "\n",
        "def run_Kmeans(num_clusters, data, max_iterations=50):\n",
        "    centroids = initialize_random_centroids(num_clusters, data)\n",
        "    for _ in range(max_iterations):\n",
        "        clusters = create_clusters(centroids, num_clusters, data)\n",
        "        previous_centroids = centroids\n",
        "        centroids = compute_means(clusters, num_clusters, data)\n",
        "        if np.allclose(previous_centroids, centroids):\n",
        "            return clusters, centroids\n",
        "    return clusters, centroids\n",
        "\n",
        "def score_within_cluster_dispersion(cluster, data_clusters):\n",
        "    cluster_data = data_clusters[cluster]\n",
        "    cluster_size = cluster_data.shape[0]\n",
        "    cluster_dispersion = np.sum(np.var(cluster_data, ddof=0, axis=0))\n",
        "    return cluster_size * cluster_dispersion\n",
        "\n",
        "def calculate_index(data):\n",
        "    data_clusters = {}\n",
        "    data_features = data.copy()    \n",
        "    label_target = 'XCoord'\n",
        "    data_target = data_features.pop(label_target)\n",
        "    labels_clusters = np.unique(data_target)\n",
        "    num_clusters = len(labels_clusters)\n",
        "\n",
        "    data_frame = data.copy()\n",
        "\n",
        "    for cluster in labels_clusters:\n",
        "        data_clusters[cluster] = data_frame[data_frame[label_target] == cluster].drop(columns=label_target)\n",
        "\n",
        "    num_observation_for_specific_cluster = {cluster: len(data_clusters[cluster])\n",
        "             for cluster in labels_clusters}\n",
        "    \n",
        "    B = pd.DataFrame()\n",
        "    data_centroids = data_frame.groupby(by=label_target).mean().T\n",
        "    data_barycenter = data_features.mean()\n",
        "\n",
        "    for cluster in labels_clusters:\n",
        "        B = B.append(np.sqrt(num_observation_for_specific_cluster[cluster]) *\n",
        "        (data_centroids[cluster] - data_barycenter), ignore_index=True)\n",
        "    \n",
        "    scatter_matrix_between_group_BG = B.T.dot(B)\n",
        "\n",
        "    score_between_group_dispersion = np.trace(scatter_matrix_between_group_BG)\n",
        "\n",
        "    BGSS_red = score_between_group_dispersion / (num_clusters - 1)\n",
        "\n",
        "    score_pooled_within_cluster_dispersion = np.sum([score_within_cluster_dispersion(cluster, data_clusters) for cluster in labels_clusters])\n",
        "    \n",
        "    num_observations = len(data_features)\n",
        "    WGSS_red = score_pooled_within_cluster_dispersion / (num_observations - num_clusters)\n",
        "\n",
        "    index = BGSS_red / WGSS_red\n",
        "\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    start_timer = timeit.default_timer()   \n",
        "\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    if rank == 0:\n",
        "        data = pd.read_csv('/content/drive/MyDrive/Datasets/brooklyn_sales_map.csv', low_memory=False)\n",
        "        data.drop(data.columns.difference(['XCoord','YCoord']), 1, inplace=True)\n",
        "        df = data[data.XCoord.notnull()]\n",
        "        coordinates = df.head(1000)\n",
        "\n",
        "        mind = [0 for i in range(size)]\n",
        "\n",
        "        for i in range(size - 1):\n",
        "            n = int(coordinates.size / size)\n",
        "            rndperm = np.random.permutation(coordinates.shape[0])\n",
        "            send_buf = coordinates.iloc[rndperm[0:500], :].copy()\n",
        "            \n",
        "            comm.send(n, dest = i + 1, tag = 13)\n",
        "            comm.send(send_buf, dest = i + 1, tag = 12)        \n",
        "            mind[i + 1] = comm.recv(tag = 12, source = i + 1)\n",
        "        \n",
        "        max_val = 0\n",
        "        max_index = 0\n",
        "        \n",
        "        for i in range(size - 1):\n",
        "            if (mind[i + 1] > max_val):\n",
        "                max_index = i + 1\n",
        "                max_val = mind[i + 1]\n",
        "\n",
        "        print('Max index: ', max_index)\n",
        "        print('Max value: ',  max_val)\n",
        "\n",
        "        for i in range(size - 1): \n",
        "            if (i + 1 == max_index):\n",
        "                comm.send(1, dest = i + 1, tag = 11)\n",
        "                centroids = comm.recv(source = i + 1, tag = 11)  \n",
        "                print('Centroids: ', centroids) \n",
        "            else:\n",
        "                comm.send(0, dest = i + 1, tag = 11)\n",
        "                ans = comm.recv(source = i + 1, tag = 11)\n",
        "\n",
        "        time = timeit.default_timer()-start_timer\n",
        "        print('Running time: {:2.4f} sec'.format(time))\n",
        "\n",
        "    else:\n",
        "        num_data = comm.recv(source=0, tag = 13)\n",
        "        coordinates = np.empty(num_data, dtype='f')\n",
        "        coordinates = comm.recv(source=0, tag = 12)\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        df = coordinates.drop(['XCoord'], axis=1)\n",
        "        df = scaler.fit_transform(df)\n",
        "        \n",
        "        kmeans, centroids = run_Kmeans(4, df)\n",
        "        ch_index = calculate_index(coordinates)\n",
        "        comm.send(ch_index, dest = 0, tag = 12)\n",
        "\n",
        "        task = comm.recv(source = 0, tag = 11)\n",
        "        if (task == 1):\n",
        "            comm.send(centroids, dest = 0, tag = 11)\n",
        "        else:\n",
        "            comm.send(0, dest = 0, tag = 11)\n",
        "\n",
        "main()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeatIDyHmGk8",
        "outputId": "0d1640dc-71de-4bba-b195-e2c24b43074f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max index:  3\n",
            "Max value:  100.60982064305024\n",
            "Centroids:  [[0.        ]\n",
            " [0.87093231]\n",
            " [0.9420562 ]\n",
            " [0.75915894]]\n",
            "Running time: 157.5249 sec\n"
          ]
        }
      ],
      "source": [
        "! mpirun -n 8 --allow-run-as-root --oversubscribe python LB4Point2Point.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}